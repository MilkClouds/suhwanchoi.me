<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Suhwan Choi</title>
  <meta name="author" content="Suhwan Choi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <!-- Header Section -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">Suhwan Choi</p>
                  <p>
                    I'm an undergraduate student majoring in Physics and Computer Science at <a
                      href="https://www.snu.ac.kr/">Seoul National University</a>. I'm currently a Principal Researcher
                    at <a href="https://maum.ai/">Maum.ai</a>, where I lead the autonomous robotics research division.
                    <br><br>
                    My main research interests are in approximating and imitating human behavior and intelligence in
                    multimodal modalities, utilizing end-to-end architectures and scalable training suites. I focus on
                    embodied AI, robotic navigation, vision-language models, and multimodal learning.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:milkclouds00@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/suhwan_choi_cv.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/suhwan-choi-5a0461204/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/MilkClouds">Github</a> &nbsp;/&nbsp;
                    <a href="https://milkclouds.work">Blog</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/suhwan_choi.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/suhwan_choi.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Research Section Header -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research & Publications</h2>
                  <p>
                    I work on embodied AI, robotic navigation, and multimodal learning. My research focuses on scaling
                    vision-action pretraining, commonsense-aware navigation systems, and vision-language model
                    improvements. Some papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications Table -->
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- D2E Publication -->
              <tr>
                <td style="padding:8px;width:100%;vertical-align:middle">
                  <a href="https://worv-ai.github.io/d2e/">
                    <span class="papertitle">D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to
                      Embodied AI</span>
                  </a>
                  <br>
                  <strong>Suhwan Choi*</strong>, Jaeyoon Jung*, Haebin Seong*, Minchan Kim, Minyeong Kim, Yongjun Cho,
                  Yoonshik Kim, Yubeen Park, Youngjae Yu†, Yunsung Lee†
                  <br>
                  <em>Under Review</em>
                  <br>
                  <a href="https://worv-ai.github.io/d2e/">project page</a>
                  <p></p>
                  <p>Scaling vision-action pretraining on desktop data enables effective transfer to embodied AI tasks.
                  </p>
                </td>
              </tr>

              <!-- CANVAS Publication -->
              <tr>
                <td style="padding:8px;width:100%;vertical-align:middle">
                  <a href="https://worv-ai.github.io/canvas/">
                    <span class="papertitle">CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot
                      Interaction</span>
                  </a>
                  <br>
                  <strong>Suhwan Choi*</strong>, Yongjun Cho*, Minchan Kim*, Jaeyoon Jung*, Myunchul Joe, Yubeen Park,
                  Minseo Kim, Sungwoong Kim, Sungjae Lee, Hwiseong Park, Jiwan Chung, Youngjae Yu†
                  <br>
                  <em>ICRA 2025</em> &nbsp; <font color="red"><strong>(Outstanding Paper Award at NeurIPS 2024 Workshop,
                      3%)</strong></font>
                  <br>
                  <a href="https://worv-ai.github.io/canvas/">project page</a>
                  <p></p>
                  <p>A commonsense-aware navigation system that enables intuitive human-robot interaction through
                    natural language understanding.</p>
                </td>
              </tr>

              <!-- ESREAL Publication -->
              <tr>
                <td style="padding:8px;width:100%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2403.16167">
                    <span class="papertitle">ESREAL: Exploiting Semantic Reconstruction to Mitigate Hallucinations in
                      Vision-Language Models</span>
                  </a>
                  <br>
                  Minchan Kim*, Minyeong Kim*, Junik Bae*, <strong>Suhwan Choi</strong>, Sungkyung Kim, Buru Chang†
                  <br>
                  <em>ECCV 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2403.16167">arXiv</a>
                  <p></p>
                  <p>Exploiting semantic reconstruction to mitigate hallucinations in vision-language models.</p>
                </td>
              </tr>

              <!-- Orthogonal Updates Publication -->
              <tr>
                <td style="padding:8px;width:100%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.11881">
                    <span class="papertitle">Revisiting Residual Connections: Orthogonal Updates for Stable and
                      Efficient Deep Networks</span>
                  </a>
                  <br>
                  Giyeong Oh, Woohyun Cho, Siyeol Kim, <strong>Suhwan Choi</strong>, Youngjae Yu†
                  <br>
                  <em>NeurIPS 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2505.11881">arXiv</a>
                  <p></p>
                  <p>Revisiting residual connections with orthogonal updates for more stable and efficient deep
                    networks.</p>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- Experience Section -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Experience</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <strong>Principal Researcher</strong> at <a href="https://maum.ai/">Maum.ai</a> (Feb 2024 – Present)
                  <ul>
                    <li>Founded autonomous robotics research division as the first researcher, leading strategic
                      decisions and team expansion to 10 researchers.</li>
                    <li>Contributed as first author to majority of research projects in robotic navigation and embodied
                      AI.</li>
                    <li>Led CORE: Slurm-based DGX Cluster construction project (96 H100 GPUs, 12 nodes). <a
                        href="https://worv.ghost.io/core-construction/">[Blog]</a></li>
                    <li>Implemented company-wide Notion workspace enhancing productivity and streamlining workflows. <a
                        href="https://milkclouds.notion.site/DBs-template-1816b908318e809e9f93ecbfe7d4ca86">[Template]</a>
                    </li>
                  </ul>
                </td>
              </tr>
              <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <strong>Machine Learning Engineer Intern</strong> at <a
                    href="https://hyperconnect.com/">Hyperconnect</a> (July 2023 – Jan 2024)
                  <ul>
                    <li>Worked on diffusion-based personalized profile image generation for real-world applications.
                    </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Awards Section -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Awards & Honors</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <strong><a href="https://qhack.ai">QHack Coding Challenge</a></strong> (2023 and 2024)
                  <ul>
                    <li>Ranked <strong>4th/793</strong> teams in 2023, Ranked <strong>3rd/618</strong> teams in 2024.
                    </li>
                    <li>Contest implementing quantum algorithms, quantum machine learning, quantum chemistry, and
                      brain-teasing puzzles.</li>
                  </ul>
                </td>
              </tr>
              <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <strong><a href="https://qhackathon.kr/">2023 Quantum Hackathon</a></strong> (2023)
                  <ul>
                    <li><strong>1st place, Minister of Science and ICT Award</strong></li>
                    <li>Topic: Utilizing symmetry to solve variational quantum algorithm (quantum machine learning)
                      efficiently.</li>
                  </ul>
                </td>
              </tr>
              <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <strong><a href="https://campaign.naver.com/clova_airush/">NAVER CLOVA AI RUSH 2022</a></strong> (July
                  – Sept 2022)
                  <ul>
                    <li><strong>3rd place</strong> on Landmark Detection (3,000,000 KRW)</li>
                    <li><strong>2nd place</strong> on Shopping User Embedding Extraction, Classification (7,000,000 KRW)
                    </li>
                  </ul>
                </td>
              </tr>
              <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <strong>Google Codejam 2022</strong> (2022)
                  <ul>
                    <li>Round 3, 546th (awarded T-Shirt).</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Open Source Contributions Section -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Open Source Contributions</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <a href="https://github.com/open-world-agents"><img src='images/owa-logo.jpg' width="160"
                      alt="Open World Agents"></a>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/open-world-agents">
                    <span class="papertitle">Open World Agents</span>
                  </a>
                  <br>
                  <p>Built comprehensive multimodal desktop agent framework including optimized data collection tool,
                    standardized efficient data format, multimedia data processing pipelines, dataset management, agent
                    training infrastructure, Python packaging, and CI/CD.</p>
                </td>
              </tr>
            </tbody>
          </table>



          <!-- Footer -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Website source code available on <a href="https://github.com/MilkClouds/suhwanchoi.me">GitHub</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>